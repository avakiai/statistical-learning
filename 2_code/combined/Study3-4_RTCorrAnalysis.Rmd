---
title: "Study3-4_RTCorrelationPatternAnalysis"
author: "Ava Kiai"
date: "9/8/2020"
output: html_document
---
Code for the final two analyses in Kiai & Melloni 2020: RSA on group-level RTs for each feature, and RSA for individuals, correlated with their word recognition performance. 

## Load & Init.
```{r}
library(tidyverse)
library(extrafont)
par(family="LM Roman 10")
library(DescTools) 
```

```{r}
#data_path
#code_path
#res_path
fig_path <- 'C:/Users/Ava/Desktop/Experiments/statistical_learning/3_results/combined'
res_path <- 'C:/Users/Ava/Desktop/Experiments/statistical_learning/3_results/combined'
#source("summarySE.R")
exps.3.4.data <- read_csv('C:/Users/Ava/Desktop/Experiments/statistical_learning/1_data/combined/exps_3_4s_data.csv', col_types = "cccdddcddddc") %>% mutate(subject = as.factor(subject),
                  cond_order = as.character(cond_order),
                  sess = as.character(sess), 
                  target = as.factor(target),
                  tgt_pos = as.factor(tgt_pos), 
                  tgt_word = as.factor(tgt_word),
                  exp = as.factor(exp))

```

## Compute Correlation Matrices
```{r}
# For each subject, compute a correlation matrix between syllables and then z transform that data. Put matrices into a list, and average them. Then transform back to rho. 
  all.list  <- list()
  all.cor.list  <- list()
  all.cor.z.list  <- list()
  all.cor.diss.z.list <- list()
  all.cor.diss.list <- list()

for (curr_subj in unique(exps.3.4.data$subject)) {
  curr_data <- exps.3.4.data %>% 
    filter(subject == curr_subj) %>%
    dplyr::select(target,rt) %>% 
    na.omit() %>%
    group_by(target) %>%
    dplyr::mutate(row_id=1:n()) %>%
    ungroup() %>%  
    spread(key = target, value = rt) %>%
    #ungroup(subject)
    dplyr::select(-row_id) 
  
# find minimum number of rows, N
  n_rows <- curr_data %>% summarise_all(funs(sum(!is.na(.)))) %>% unlist() %>% unname()
# exceptions
  if(curr_subj == "s1911") { # 3/29
    curr_data <- curr_data %>% 
    mutate(tu = as.numeric(rep(NA,nrow(curr_data))))
    N <- sort(n_rows)[2] # select the second lowest length
  } else if(curr_subj == "s1107") { # 3/26
    # subject had only 2 entires in tu, so just make the column NA to preserve other data
    curr_data <- curr_data %>% 
    mutate(tu = as.numeric(rep(NA,nrow(curr_data))))
    N <- sort(n_rows)[2] # select the second lowest length
  } else if (curr_subj == "l2805") { # 3/21
    curr_data <- curr_data %>% 
    mutate(be = as.numeric(rep(NA,nrow(curr_data))))
    N <- sort(n_rows)[2] 
  } else if (curr_subj == "h0207") { # 3/11
    curr_data <- curr_data %>% 
    add_column(ro = as.numeric(rep(NA,nrow(curr_data))))
    n_rows <- curr_data %>% summarise_all(funs(sum(!is.na(.)))) %>% unlist() %>% unname()
    N <- sort(n_rows)[2]
  } else if (curr_subj == "g0609") { # 3/8
    curr_data <- curr_data %>% 
    mutate(po = as.numeric(rep(NA,nrow(curr_data))))
    N <- sort(n_rows)[2] # select the second lowest length
    # and maybe also ga? 
  } else if (curr_subj == "b0207") { # 3/2
    curr_data <- curr_data %>% 
    add_column(be = as.numeric(rep(NA,nrow(curr_data))),.after = "za") %>%
    add_column(ro = as.numeric(rep(NA,nrow(curr_data))),.after = "di")
    n_rows <- curr_data %>% summarise_all(funs(sum(!is.na(.)))) %>% unlist() %>% unname()
    N <- sort(n_rows)[3]
  } else if (curr_subj == "a0604") { # 3/1
    curr_data <- curr_data %>% 
    mutate(be = as.numeric(rep(NA,nrow(curr_data))))
    N <- sort(n_rows)[2] 
  } else if (curr_subj == "s2205") { # 4/16    
    curr_data <- curr_data %>% 
    add_column(be = as.numeric(rep(NA,nrow(curr_data))),.after = "za") %>%
    add_column(se = as.numeric(rep(NA,nrow(curr_data))),.after = "ki") %>%
    mutate(za = as.numeric(rep(NA,nrow(curr_data)))) %>%
    mutate(ga = as.numeric(rep(NA,nrow(curr_data))))
    n_rows <- curr_data %>% summarise_all(funs(sum(!is.na(.)))) %>% unlist() %>% unname()
    N <- sort(n_rows)[5]
  } else if (curr_subj == "s0310") { # 4/12
    # subject is missing be data, so just make it NA, N is the same
    curr_data <- curr_data %>% 
    add_column(be = as.numeric(rep(NA,nrow(curr_data))),.after = "za")
    n_rows <- curr_data %>% summarise_all(funs(sum(!is.na(.)))) %>% unlist() %>% unname()
    N <- sort(n_rows)[2]
  } else if (curr_subj == "a2605") { # 4/2
    curr_data <- curr_data %>% 
    mutate(be = as.numeric(rep(NA,nrow(curr_data)))) %>%
    mutate(tu = as.numeric(rep(NA,nrow(curr_data))))
    N <- sort(n_rows)[3]
  } else {
    N <- min(n_rows)
  }
# create output matrix with N rows
  curr_data.mat <- curr_data[1:N,]
# shuffle longer columns into shorter new matrix
  for (i in 1:length(curr_data.mat)) {
  if (n_rows[i] < N) {curr_data.mat[,i] <- sample(as.matrix(curr_data[1:N,i]), size = N)
  } else {curr_data.mat[,i] <- sample(as.matrix(curr_data[1:n_rows[i],i]), size = N)}
  }

  
# take correlation(), then transform to z values()
  curr_data.cor <- cor(curr_data.mat) # corr
  curr_data.cor.z <- FisherZ(cor(curr_data.mat)) # corr - fisher
  curr_data.cor.diss.z <- FisherZ(1-cor(curr_data.mat)) # diss - fisher
  curr_data.cor.diss <- 1-(cor(curr_data.mat)) # diss

# append
  all.list <- append(all.list, list(curr_data.mat)) # Regular matrices, uneven
  all.cor.list <- append(all.cor.list, list(curr_data.cor)) # Correlations
  all.cor.z.list <- append(all.cor.z.list, list(curr_data.cor.z)) # Z-transformed correlations
  all.cor.diss.z.list <- append(all.cor.diss.z.list, list(curr_data.cor.diss.z)) # Z-transformed 1-correlations
  all.cor.diss.list <- append(all.cor.diss.list, list(curr_data.cor.diss)) # 1-correlations
}


# Fisher Z merely makes the sampling distribution more normal to make calculation easier, but
# it's not required if the computation is performed on a computer that can handle the exact 
# (however skewed) true distribution: https://stats.stackexchange.com/questions/420142/why-is-fisher-transformation-necessary

# For the z's, the diag is infinite
# all.cor <- apply(simplify2array(all.cor.list), 1:2, mean, na.rm= TRUE)
# all.z <- apply(simplify2array(all.cor.z.list), 1:2, mean, na.rm= TRUE)
# The results of these two are nearly identical -- identical out to about 3 digits, but indeed the z's are normal-ish

# The problem with all.cor.diss.z.list is that some 1-corr values are > 1, which is incalculable for the z transformation. Thus, many
# values become NA. 
```
To avoid data loss due to particularities of the Fisher Z transformation, and to normalize the distribution of correlation values, we will perform the RSA on z-transformed similarity (corrlation) values. 

# Group-Level Analysis
Procedure: 
1. Take 1-correlations for each subject, z transform
#### Set analysis type
```{r}
# Group level analysis on this matrix: 

# Dissimilarity Matrix
# group.analysis <- all.cor.diss.list
# name <- 'diss'
# label <- 'dissimilarity (1-'

# group.analysis <- all.cor.diss.z.list
# name <- 'diss_z'
# label <- 'dissimilarity z(1-'

# Similarity Matrix
# group.analysis <- all.cor.list
# name <- 'corr'
# label <- 'similarity ('

group.analysis <- all.cor.z.list
name <- 'corr_z'
label <- 'similarity z('

```

2. Sample 200 times with replacement from all participant's matrices for each within vs. across category; subsample arrays to make them equal in length.
3. Compute wilcoxon

For each test, we look at the correlation between items in each category vs. items outside that category.


#### Set Seed
```{r}
set.seed(42)
```

### - Ordinal Position
Within: 
1s: (all ones vs. all other ones) nu vs. ro, nu vs. mi, etc.  
2s: (all 2s vs. all other twos) ga vs. ki, etc. 
3s: (all threes vs. all other threes) di vs. se, etc. 
... together 

Across:
1-2s, 2-3s: (crossed positions within words) nu vs. ga, ga vs. di, nu vs. di | basically, word identity
```{r}
# indices for each matrix
wn.OP <- list(c(1,4),c(1,7),c(1,10),c(4,7),c(4,10),c(7,10), # 1s
             c(2,5),c(2,8),c(2,11),c(5,8),c(5,11),c(8,11), #2s
             c(3,6),c(3,9),c(3,12),c(6,9),c(6,12),c(9,12)) #3s
ac.OP <- list(c(1,2),c(2,3),c(1,3),
              c(4,5),c(5,6),c(4,6), 
             c(7,8),c(7,9),c(8,9),
             c(10,11),c(11,12),c(10,12)) 

# collect values for each of these boxes from all participants (all matrices in the list)
# You can check what's happening here like so: group.analysis[[1]][wn.OP[[i]][1],wn.OP[[i]][2]]
wn.arr <- list()
for (i in 1:length(wn.OP)) {wn.arr <- append(wn.arr, mapply(function(x, y) x[y][2], group.analysis, wn.OP[i], SIMPLIFY = FALSE))} 
ac.arr <- list()
for (i in 1:length(ac.OP)) {ac.arr <- append(ac.arr, mapply(function(x, y) x[y][2], group.analysis, ac.OP[i], SIMPLIFY = FALSE))} 

within.OP <- cbind(wn.arr) %>% as.numeric()
within.OP[mapply(is.infinite, within.OP)] <- NA # clean, in case using z-transform
loss <- length(within.OP[is.na(within.OP)])/length(within.OP)
if (loss > 0.1) {stop("Within OP loss too high.")}
within.OP <- within.OP[!is.na(within.OP)] # remove NA

across.OP <- cbind(ac.arr) %>% as.numeric()
across.OP[mapply(is.infinite, across.OP)] <- NA
loss <- length(across.OP[is.na(across.OP)])/length(across.OP) 
if (loss > 0.1) {stop("Across OP loss too high.")}
across.OP <- across.OP[!is.na(across.OP)] # remove NA

# compute maximum length
(n.op <- round(min(length(within.OP),length(across.OP))*(4/5)))

# sample & bootstrap to produce a matrix and bootstrapped means
within.OP.samp <- as.numeric(replicate(200, sample(within.OP, size = n.op, replace = TRUE)))
within.OP.means <- replicate(200, mean(sample(within.OP, size = n.op, replace = TRUE), na.rm = TRUE)) 

across.OP.samp <- as.numeric(replicate(200, sample(across.OP, size = n.op, replace = TRUE)))
across.OP.means <- replicate(200, mean(sample(across.OP, size = n.op, replace = TRUE), na.rm = TRUE)) 


(wt.OP <- wilcox.test(within.OP.samp,across.OP.samp, conf.int = TRUE, conf.level = 0.95, paired = TRUE))
```


### - Transitional Probability
Within:
.33's: same as first row for Ordinal Position

Across: 
1's: same as second two rows for Ordinal Position

```{r}

wn.TP <- list(c(1,4),c(1,7),c(1,10),c(4,7),c(4,10),c(7,10)) # low TP
ac.TP <- list(c(2,5),c(2,8),c(2,11),c(5,8),c(5,11),c(8,11), # high TP
             c(3,6),c(3,9),c(3,12),c(6,9),c(6,12),c(9,12))

wn.tp.arr <- list()
for (i in 1:length(wn.TP)) {
    wn.tp.arr <- append(wn.tp.arr, mapply(function(x, y) x[y][2], group.analysis, wn.TP[i], SIMPLIFY = FALSE))
} 

ac.tp.arr <- list()
for (i in 1:length(ac.TP)) {
    ac.tp.arr <- append(ac.tp.arr, mapply(function(x, y) x[y][2], group.analysis, ac.TP[i], SIMPLIFY = FALSE))
} 

within.TP <- cbind(wn.tp.arr) %>% as.numeric()
within.TP[mapply(is.infinite, within.TP)] <- NA
loss <- length(within.TP[is.na(within.TP)])/length(within.TP)
if (loss > 0.1) {print(paste0("Within TP loss: ",round(loss, digits = 3),"%."))
  #stop("Within TP loss too high.")
  }
within.TP <- within.TP[!is.na(within.TP)] # remove NA


across.TP <- cbind(ac.tp.arr) %>% as.numeric()
across.TP[mapply(is.infinite, across.TP)] <- NA
loss <- length(across.TP[is.na(across.TP)])/length(across.TP)
if (loss > 0.1) {print(paste0("Across TP loss: ",round(loss, digits = 3),"%."))
  #stop("Across TP loss too high.")
  }
across.TP <- across.TP[!is.na(across.TP)] # remove NA

(n.tp <- round(min(length(within.TP),length(across.TP))*(4/5)))

within.TP.samp <- replicate(200, sample(within.TP, size = n.tp, replace = TRUE)) 
within.TP.means <- replicate(200, mean(sample(within.TP, size = n.tp, replace = TRUE), na.rm = TRUE)) 

across.TP.samp <- replicate(200, sample(across.TP, size = n.tp, replace = TRUE)) 
across.TP.means <- replicate(200, mean(sample(across.TP, size = n.tp, replace = TRUE), na.rm = TRUE)) 


(wt.TP <- wilcox.test(within.TP.samp,across.TP.samp, conf.int = TRUE, conf.level = 0.95, paired = TRUE))

```

### - Triplet Identity
Within: all comparisons within a words of the form 1-2, 2-3, 1-3
nu-ga, ga-di, nu-di
across: "phantom word" comparisons of the form 1-2,2-3,1-3, but only across words
nu-ki, nu-se
```{r}
wn.WI <- list(c(1,2),c(2,3),c(1,3),# nugadi
              c(4,5),c(5,6),c(4,6),# rokise
             c(7,8),c(7,9),c(8,9),# mipola
             c(10,11),c(11,12),c(10,12))# zabetu
ac.WI <- list(c(1,5),c(2,6),c(1,6),c(1,8),c(2,9),c(1,9),c(1,11),c(2,12),c(1,12),# nu-ki, ga-se, nu-se, nu-po, ga-la, nu-la, nu-be, ga-tu, nu-tu 
              c(4,2),c(5,3),c(4,3),c(4,8),c(5,9),c(4,9),c(4,11),c(5,12),c(4,12), # ro-ga, ki-di, ro-di, ro-po, ki-la, ro-la, ro-be, ki-tu, ro-tu 
              c(7,2),c(8,3),c(7,3),c(7,5),c(8,6),c(7,6),c(7,11),c(8,12),c(7,12), # mi-ga, po-di, mi-di, mi-ki, po-se, mi-se, mi-be, po-tu, mi-tu
              c(10,2),c(11,3),c(10,3),c(11,5),c(10,6),c(11,6),c(10,8),c(11,9),c(10,9)) # za-ga, be-di, za-di, za-ki, be-ki, za-se, za-po, be-la, za-la 

wn.WI.arr <- list()
for (i in 1:length(wn.WI)) {
    wn.WI.arr <- append(wn.WI.arr, mapply(function(x, y) x[y][2], group.analysis, wn.WI[i], SIMPLIFY = FALSE))
} 

ac.WI.arr <- list()
for (i in 1:length(ac.WI)) {
    ac.WI.arr <- append(ac.WI.arr, mapply(function(x, y) x[y][2], group.analysis, ac.WI[i], SIMPLIFY = FALSE))
} 

within.WI <- cbind(wn.WI.arr) %>% as.numeric()
within.WI[mapply(is.infinite, within.WI)] <- NA
loss <- length(within.WI[is.na(within.WI)])/length(within.WI)
if (loss > 0.1) {print(paste0("Within WI loss: ",round(loss, digits = 3),"%."))
  #stop("Within WI loss too high.")
  }
within.WI <- within.WI[!is.na(within.WI)] # remove NA

across.WI <- cbind(ac.WI.arr) %>% as.numeric()
across.WI[mapply(is.infinite, across.WI)] <- NA
loss <- length(across.WI[is.na(across.WI)])/length(across.WI)
if (loss > 0.1) {print(paste0("Across WI loss: ",round(loss, digits = 3),"%."))
 # stop("Across WI loss too high.")
  }
across.WI <- across.WI[!is.na(across.WI)] # remove NA

(n.wi <- round(min(length(within.WI),length(across.WI))*(4/5)))

within.WI.samp <- replicate(200, sample(within.WI, size = n.wi, replace = TRUE)) 
within.WI.means <- replicate(200, mean(sample(within.WI, size = n.wi, replace = TRUE), na.rm = TRUE)) 

across.WI.samp <- replicate(200, sample(across.WI, size = n.wi, replace = TRUE)) 
across.WI.means <- replicate(200, mean(sample(across.WI, size = n.wi, replace = TRUE), na.rm = TRUE)) 

(wt.WI <- wilcox.test(within.WI.samp,across.WI.samp, conf.int = TRUE, conf.level = 0.95, paired = TRUE))
```


### - Duplet Identity
Within: All proper duplets, 1-2s, 2-3s
Across: 1-3's
```{r}
wn.di <- list(c(1,2),c(2,3),
              c(4,5),c(5,6),
             c(7,8),c(7,9),
             c(10,11),c(11,12))
ac.di <- list(c(1,3),c(4,6),c(8,9),c(10,12))


wn.di.arr <- list()
for (i in 1:length(wn.di)) {
    wn.di.arr <- append(wn.di.arr, mapply(function(x, y) x[y][2], group.analysis, wn.di[i], SIMPLIFY = FALSE))
} 

ac.di.arr <- list()
for (i in 1:length(ac.di)) {
    ac.di.arr <- append(ac.di.arr, mapply(function(x, y) x[y][2], group.analysis, ac.di[i], SIMPLIFY = FALSE))
} 

within.di <- cbind(wn.di.arr) %>% as.numeric()
within.di[mapply(is.infinite, within.di)] <- NA
loss <- length(within.di[is.na(within.di)])/length(within.di)
if (loss > 0.1) {print(paste0("Within DI loss: ",round(loss, digits = 3),"%."))
 # stop("Within DI loss too high.")
  }
within.di <- within.di[!is.na(within.di)] # remove NA

across.di <- cbind(ac.di.arr) %>% as.numeric()
across.di[mapply(is.infinite, across.di)] <- NA
loss <- length(across.di[is.na(across.di)])/length(across.di)
if (loss > 0.1) {print(paste0("Across DI loss: ",round(loss, digits = 3),"%."))
  #stop("Across DI loss too high.")
  }
across.di <- across.di[!is.na(across.di)] # remove NA

(n.di <- round(min(length(within.di),length(across.di))*(4/5)))

within.di.samp <- replicate(200, sample(within.di, size = n.di, replace = TRUE))
within.di.means <- replicate(200, mean(sample(within.di, size = n.di, replace = TRUE), na.rm = TRUE))

across.di.samp <- replicate(200, sample(across.di, size = n.di, replace = TRUE))
across.di.means <- replicate(200, mean(sample(across.di, size = n.di, replace = TRUE), na.rm = TRUE))

(wt.DI <- wilcox.test(within.di.samp,across.di.samp, conf.int = TRUE, conf.level = 0.95, paired = TRUE))

```

This was essentially 4 Mann-Whitney Tests:  if both x and y are given and paired is FALSE, a Wilcoxon rank sum test (equivalent to the Mann-Whitney test: see the Note) is carried out. In this case, the null hypothesis is that the distributions of x and y differ by a location shift of mu and the alternative is that they differ by some other location shift (and the one-sided alternative "greater" is that x is shifted to the right of y).

P-values computed via normal approximation.

Note that in the two-sample case the estimator for the difference in location parameters does not estimate the difference in medians (a common misconception) but rather the median of the difference between a sample from x and a sample from y.

### -> Combine Data/Wilcoxon
```{r}

WA.wilcox <- data.frame(test = factor(c(
"ordinal position",
"transitional probability",
"word identity",
"duplets"), levels = c("ordinal position","transitional probability","word identity","duplets")),
mean = as.numeric(c(mean(within.OP.means)-mean(across.OP.means),
mean(within.TP.means)-mean(across.TP.means),
mean(within.WI.means)-mean(across.WI.means),
mean(within.di.means)-mean(across.di.means))),
mean_diff = as.numeric(c(
wt.OP$estimate,
wt.TP$estimate,
wt.WI$estimate,
wt.DI$estimate)),
N = as.numeric(c(
n.op,
n.tp,
n.wi,
n.di)),
w = as.numeric(c(
wt.OP$statistic,
wt.TP$statistic,
wt.WI$statistic,
wt.DI$statistic)),
"CI low" = as.numeric(c(
wt.OP$conf.int[1],
wt.TP$conf.int[1],
wt.WI$conf.int[1],
wt.DI$conf.int[1])),
"CI hi" = as.numeric(c(
wt.OP$conf.int[2],
wt.TP$conf.int[2],
wt.WI$conf.int[2],
wt.DI$conf.int[2])),
p = as.numeric(c(
wt.OP$p.value,
wt.TP$p.value,
wt.WI$p.value,
wt.DI$p.value)))

#write.csv(WA.wilcox,file.path(res_path,paste0('/wilcox_',name,'_run_x.csv')), row.names = FALSE)

```

### -> Plot
```{r}
WA.wilcox <- read_csv(file.path(res_path,paste0('/wilcox_',name,'_run_x.csv'))) %>%
  mutate(test = factor(test,levels = c("ordinal position","transitional probability","word identity","duplets")))
```


```{r}
ggplot(WA.wilcox) + 
  geom_col(aes(test,mean_diff,fill=test), color = "black") + #, fill = "grey", color = "black") + 
  geom_hline(yintercept = 0, color = "black", linetype="dashed") + 
  scale_x_discrete(name = NULL) + 
  scale_y_continuous(name = paste0("within - across ", label ,"Pearson's r)")) +
  scale_fill_manual(values = c("#D95C4E", "#E8B651", "#7BB095", "#5E8FEB"), guide = NULL) +
  annotate(geom = "text", x = 1, y = 0.052, size = 4, label = "***") +
  annotate(geom = "text", x = 2, y = 0.09, size = 4, label = "***") +
  annotate(geom = "text", x = 4, y = 0.058, size = 4, label = "***") +
  # annotate(geom = "text", x = 1, y = 0.055, size = 4, label = "p<0.0001\n***") +
  # annotate(geom = "text", x = 2, y = 0.094, size = 4, label = "p<0.0001\n***") +
  # annotate(geom = "text", x = 4, y = 0.062, size = 4, label = "p<0.0001\n***") +
  theme_classic() + 
  theme(text = element_text(family = "LM Roman 10", face="bold", size = 15)) +
  ggsave(file.path(fig_path, paste0('/fig9_',name,'_run_x_stars_col.png')), width = 9, height = 5)  

```



# Subject-Level Analysis

### Recompute Correlation Matrices for Experiment 3 only
The subjects used in the previous analysis were from both exp's 3 and 4. Here, we want only those from experiment 3, so that we can compare their rt patterns with their offline word recognition performance. We'll re-run the analysis above but separate out the kids from the first experiment first. 
```{r eval=FALSE, warning=FALSE, include=FALSE}
# For each subject, compute a correlation matrix between syllables and then z transform that data. Put matrices into a list, and average them. Then transform back to rho. 
  exp3.list  <- list()
  exp3.cor.list  <- list()
  exp3.cor.z.list  <- list()
  exp3.cor.diss.z.list <- list()
  exp3.cor.diss.list <- list()

exp.3.only <- exps.3.4.data %>% filter(exp == "exp 3")
  
for (curr_subj in unique(exp.3.only$subject)) {
  curr_data <- exp.3.only %>% 
    filter(subject == curr_subj) %>%
    dplyr::select(target,rt) %>% 
    na.omit() %>%
    group_by(target) %>%
    dplyr::mutate(row_id=1:n()) %>%
    ungroup() %>%  
    spread(key = target, value = rt) %>%
    #ungroup(subject)
    dplyr::select(-row_id) 
  
# find minimum number of rows, N
  n_rows <- curr_data %>% summarise_all(funs(sum(!is.na(.)))) %>% unlist() %>% unname()
# exceptions
  if(curr_subj == "s1911") { # 3/29
    curr_data <- curr_data %>% 
    mutate(tu = as.numeric(rep(NA,nrow(curr_data))))
    N <- sort(n_rows)[2] # select the second lowest length
  } else if(curr_subj == "s1107") { # 3/26
    # subject had only 2 entires in tu, so just make the column NA to preserve other data
    curr_data <- curr_data %>% 
    mutate(tu = as.numeric(rep(NA,nrow(curr_data))))
    N <- sort(n_rows)[2] # select the second lowest length
  } else if (curr_subj == "l2805") { # 3/21
    curr_data <- curr_data %>% 
    mutate(be = as.numeric(rep(NA,nrow(curr_data))))
    N <- sort(n_rows)[2] 
  } else if (curr_subj == "h0207") { # 3/11
    curr_data <- curr_data %>% 
    add_column(ro = as.numeric(rep(NA,nrow(curr_data))))
    n_rows <- curr_data %>% summarise_all(funs(sum(!is.na(.)))) %>% unlist() %>% unname()
    N <- sort(n_rows)[2]
  } else if (curr_subj == "g0609") { # 3/8
    curr_data <- curr_data %>% 
    mutate(po = as.numeric(rep(NA,nrow(curr_data))))
    N <- sort(n_rows)[2] # select the second lowest length
    # and maybe also ga? 
  } else if (curr_subj == "b0207") { # 3/2
    curr_data <- curr_data %>% 
    add_column(be = as.numeric(rep(NA,nrow(curr_data))),.after = "za") %>%
    add_column(ro = as.numeric(rep(NA,nrow(curr_data))),.after = "di")
    n_rows <- curr_data %>% summarise_all(funs(sum(!is.na(.)))) %>% unlist() %>% unname()
    N <- sort(n_rows)[3]
  } else if (curr_subj == "a0604") { # 3/1
    curr_data <- curr_data %>% 
    mutate(be = as.numeric(rep(NA,nrow(curr_data))))
    N <- sort(n_rows)[2] 
  } else {
    N <- min(n_rows)
  }
# create output matrix with N rows
  curr_data.mat <- curr_data[1:N,]
# shuffle longer columns into shorter new matrix
  for (i in 1:length(curr_data.mat)) {
  if (n_rows[i] < N) {curr_data.mat[,i] <- sample(as.matrix(curr_data[1:N,i]), size = N)
  } else {curr_data.mat[,i] <- sample(as.matrix(curr_data[1:n_rows[i],i]), size = N)}
  }

# take correlation(), then transform to z values()
  curr_data.cor <- cor(curr_data.mat) # corr
  curr_data.cor.z <- FisherZ(cor(curr_data.mat)) # corr - fisher
  curr_data.cor.diss.z <- FisherZ(1-cor(curr_data.mat)) # diss - fisher
  curr_data.cor.diss <- 1-(cor(curr_data.mat)) # diss

# append
  exp3.list <- append(exp3.list, list(curr_data.mat)) # Regular matrices, uneven
  exp3.cor.list <- append(exp3.cor.list, list(curr_data.cor)) # Correlations
  exp3.cor.z.list <- append(exp3.cor.z.list, list(curr_data.cor.z)) # Z-transformed correlations
  exp3.cor.diss.z.list <- append(exp3.cor.diss.z.list, list(curr_data.cor.diss.z)) # Z-transformed 1-correlations
  exp3.cor.diss.list <- append(exp3.cor.diss.list, list(curr_data.cor.diss)) # 1-correlations
}

```

Procedure: 
1. Take 1-correlations for each subject, z transform
### Set analysis type
```{r eval=FALSE, include=FALSE}
# Dissimilarity Matrix
# single.analysis <- exp3.cor.diss.list
# single.analysis <- exp3.cor.diss.z.list

# Similarity Matrix
# single.analysis <- exp3.cor.list
 single.analysis <- exp3.cor.z.list

 
```

```{r}
subj_table <- data.frame(subject = as.double(0),
  test = as.factor(0),
  n.with = as.double(0),
  n.accr = as.double(0),
  mean.with = as.double(0),
  mean.accr = as.double(0),
  score = as.double(0))#,
  # w = as.double(0),
  # CI.low = as.double(0),
  # CI.hi = as.double(0),
  # min_p = as.double(0),
  # p = as.double(0))
```

### Compute RSA for all positions
```{r eval=FALSE, include=FALSE}

for (curr.subj in 1:length(single.analysis)) {
  curr.subj.mat <- single.analysis[curr.subj][[1]]

# ------ Ordinal Position
# Determine cells
wn.OP <- list(c(1,4),c(1,7),c(1,10),c(4,7),c(4,10),c(7,10), # 1s
             c(2,5),c(2,8),c(2,11),c(5,8),c(5,11),c(8,11), #2s
             c(3,6),c(3,9),c(3,12),c(6,9),c(6,12),c(9,12)) #3s
ac.OP <- list(c(1,2),c(2,3),c(1,3),
              c(4,5),c(5,6),c(4,6), 
             c(7,8),c(7,9),c(8,9),
             c(10,11),c(11,12),c(10,12)) 
#print(paste("TP min p =", 2*((1/2)^12)))

# Collect all possible values
wn.arr <- list()
for (i in 1:length(wn.OP)) {wn.arr <- append(wn.arr, curr.subj.mat[unlist(wn.OP[i])[1],unlist(wn.OP[i])][2])} 
ac.arr <- list()
for (i in 1:length(ac.OP)) {ac.arr <- append(ac.arr, curr.subj.mat[unlist(ac.OP[i])[1],unlist(ac.OP[i])][2])} 
# Wrangle
within.OP <- cbind(wn.arr) %>% as.numeric()
within.OP[mapply(is.infinite, within.OP)] <- NA
#loss <- length(within.OP[is.na(within.OP)])/length(within.OP)
#if (loss > 0.2) {print(paste0("Within OP loss: ",round(loss, digits = 3),"%."))}
within.OP <- within.OP[!is.na(within.OP)] # remove NA
# - 
across.OP <- cbind(ac.arr) %>% as.numeric()
across.OP[mapply(is.infinite, across.OP)] <- NA
#loss <- length(across.OP[is.na(across.OP)])/length(across.OP)
#if (loss > 0.2) {print(paste0("Across OP loss: ",round(loss, digits = 3),"%."))}
across.OP <- across.OP[!is.na(across.OP)] # remove NA
# Compute maximum length
#(n.op <- round(min(length(within.OP),length(across.OP))))
#if(n.op < 2*((1/2)^12)) {"OP min p > 0.0004."}

# Bootstrap/permute
within.OP.mean <- mean(within.OP) 
across.OP.mean <- mean(across.OP)


normed.mu <- within.OP.mean/across.OP.mean

subj_table <- rbind(subj_table, data.frame(subject = curr.subj,
                                           test = 'ordinal position',
                                           n.with = length(within.OP),
                                           n.accr = length(across.OP),
                                           mean.with = within.OP.mean,
                                           mean.accr = across.OP.mean,
                                           score = normed.mu))
                                           # w = wt.OP$statistic,
                                           # CI.low = wt.OP$conf.int[1],
                                           # CI.hi = wt.OP$conf.int[2],
                                           # min_p = 2*((1/2)^n.op),
                                           # p = wt.OP$p.value))
# ---------------- Transitional Probability
wn.TP <- list(c(1,4),c(1,7),c(1,10),c(4,7),c(4,10),c(7,10)) # low TP
ac.TP <- list(c(2,5),c(2,8),c(2,11),c(5,8),c(5,11),c(8,11), # high TP
             c(3,6),c(3,9),c(3,12),c(6,9),c(6,12),c(9,12))
#print(paste("TP min p =", 2*((1/2)^6)))

wn.tp.arr <- list()
for (i in 1:length(wn.TP)) {wn.tp.arr <- append(wn.tp.arr, curr.subj.mat[unlist(wn.TP[i])[1],unlist(wn.TP[i])][2])} 
ac.tp.arr <- list()
for (i in 1:length(ac.TP)) {ac.tp.arr <- append(ac.tp.arr, curr.subj.mat[unlist(ac.TP[i])[1],unlist(ac.TP[i])][2])}
within.TP <- cbind(wn.tp.arr) %>% as.numeric()
within.TP[mapply(is.infinite, within.TP)] <- NA
#loss <- length(within.TP[is.na(within.TP)])/length(within.TP)
#if (loss > 0.2) {print(paste0("Within TP loss: ",round(loss, digits = 3),"%."))}
within.TP <- within.TP[!is.na(within.TP)] # remove NA
across.TP <- cbind(ac.tp.arr) %>% as.numeric()
across.TP[mapply(is.infinite, across.TP)] <- NA
#loss <- length(across.TP[is.na(across.TP)])/length(across.TP)
#if (loss > 0.2) {print(paste0("Across TP loss: ",round(loss, digits = 3),"%."))}
across.TP <- across.TP[!is.na(across.TP)] # remove NA

#(n.tp <- round(min(length(within.TP),length(across.TP))))
#if(n.tp < 2*((1/2)^6)) {"TP min p > 0.03."}

#within.TP.samp <- sample(within.TP, size = n.tp, replace = FALSE)
within.TP.mean <- mean(within.TP) 

#across.TP.samp <- sample(across.TP, size = n.tp, replace = FALSE)
across.TP.mean <- mean(across.TP) 

#(wt.TP <- wilcox.test(within.TP.samp,across.TP.samp, conf.int = TRUE, conf.level = 0.95, paired = TRUE))
normed.mu <- within.TP.mean/across.TP.mean

subj_table <- rbind(subj_table, data.frame(subject = curr.subj,
                                           test = 'transitional probability',
                                           n.with = length(within.TP),
                                           n.accr = length(across.TP),
                                           mean.with = within.TP.mean,
                                           mean.accr = across.TP.mean,
                                           score = normed.mu))
                                           # w = wt.TP$statistic,
                                           # CI.low = wt.TP$conf.int[1],
                                           # CI.hi = wt.TP$conf.int[2],
                                           # min_p = 2*((1/2)^n.tp),
                                           # p = wt.TP$p.value))

# ---------------- Word Identity
wn.WI <- list(c(1,2),c(2,3),c(1,3),# nugadi
              c(4,5),c(5,6),c(4,6),# rokise
             c(7,8),c(7,9),c(8,9),# mipola
             c(10,11),c(11,12),c(10,12))# zabetu
ac.WI <- list(c(1,5),c(2,6),c(1,6),c(1,8),c(2,9),c(1,9),c(1,11),c(2,12),c(1,12),# nu-ki, ga-se, nu-se, nu-po, ga-la, nu-la, nu-be, ga-tu, nu-tu 
              c(4,2),c(5,3),c(4,3),c(4,8),c(5,9),c(4,9),c(4,11),c(5,12),c(4,12), # ro-ga, ki-di, ro-di, ro-po, ki-la, ro-la, ro-be, ki-tu, ro-tu 
              c(7,2),c(8,3),c(7,3),c(7,5),c(8,6),c(7,6),c(7,11),c(8,12),c(7,12), # mi-ga, po-di, mi-di, mi-ki, po-se, mi-se, mi-be, po-tu, mi-tu
              c(10,2),c(11,3),c(10,3),c(11,5),c(10,6),c(11,6),c(10,8),c(11,9),c(10,9)) # za-ga, be-di, za-di, za-ki, be-ki, za-se, za-po, be-la, za-la 
#print(paste("WI min p =", 2*((1/2)^12)))

wn.WI.arr <- list()
for (i in 1:length(wn.WI)) {wn.WI.arr <- append(wn.WI.arr, curr.subj.mat[unlist(wn.WI[i])[1],unlist(wn.WI[i])][2])} 
ac.WI.arr <- list()
for (i in 1:length(ac.WI)) {ac.WI.arr <- append(ac.WI.arr, curr.subj.mat[unlist(ac.WI[i])[1],unlist(ac.WI[i])][2])}
within.WI <- cbind(wn.WI.arr) %>% as.numeric()
within.WI[mapply(is.infinite, within.WI)] <- NA
#loss <- length(within.WI[is.na(within.WI)])/length(within.WI)
#if (loss > 0.2) {print(paste0("Within WI loss: ",round(loss, digits = 3),"%."))}
within.WI <- within.WI[!is.na(within.WI)] # remove NA
across.WI <- cbind(ac.WI.arr) %>% as.numeric()
across.WI[mapply(is.infinite, across.WI)] <- NA
#loss <- length(across.WI[is.na(across.WI)])/length(across.WI)
#if (loss > 0.2) {print(paste0("Across WI loss: ",round(loss, digits = 3),"%."))}
across.WI <- across.WI[!is.na(across.WI)] # remove NA

#(n.wi <- round(min(length(within.WI),length(across.WI))))
#if(n.wi < 2*((1/2)^12)) {"OP min p > 0.0004."}

within.WI.mean <- mean(within.WI)

across.WI.mean <- mean(across.WI)

#(wt.WI <- wilcox.test(within.WI.samp,across.WI.samp, conf.int = TRUE, conf.level = 0.95, paired = TRUE))

normed.mu <- within.WI.mean/across.WI.mean

subj_table <- rbind(subj_table, data.frame(subject = curr.subj,
                                           test = 'word identity',
                                           n.with = length(within.WI),
                                           n.accr = length(across.WI),
                                           mean.with = within.WI.mean,
                                           mean.accr = across.WI.mean,
                                           score = normed.mu))
                                           # w = wt.WI$statistic,
                                           # CI.low = wt.WI$conf.int[1],
                                           # CI.hi = wt.WI$conf.int[2],
                                           # min_p = 2*((1/2)^n.wi),
                                           # p = wt.WI$p.value))
# ------------------- Duplet Identity
wn.di <- list(c(1,2),c(2,3),
              c(4,5),c(5,6),
             c(7,8),c(7,9),
             c(10,11),c(11,12))
ac.di <- list(c(1,3),c(4,6),c(8,9),c(10,12))
#print(paste("DI min p =", 2*((1/2)^4)))

wn.di.arr <- list()
for (i in 1:length(wn.di)) {wn.di.arr <- append(wn.di.arr, curr.subj.mat[unlist(wn.di[i])[1],unlist(wn.di[i])][2])} 
ac.di.arr <- list()
for (i in 1:length(ac.di)) {ac.di.arr <- append(ac.di.arr, curr.subj.mat[unlist(ac.di[i])[1],unlist(ac.di[i])][2])} 

within.di <- cbind(wn.di.arr) %>% as.numeric()
within.di[mapply(is.infinite, within.di)] <- NA
#loss <- length(within.di[is.na(within.di)])/length(within.di)
#if (loss > 0.2) {print(paste0("Within DI loss: ",round(loss, digits = 3),"%."))}
within.di <- within.di[!is.na(within.di)] # remove NA

across.di <- cbind(ac.di.arr) %>% as.numeric()
across.di[mapply(is.infinite, across.di)] <- NA
#loss <- length(across.di[is.na(across.di)])/length(across.di)
#if (loss > 0.2) {print(paste0("Across DI loss: ",round(loss, digits = 3),"%."))}
across.di <- across.di[!is.na(across.di)] # remove NA

#(n.di <- round(min(length(within.di),length(across.di))))
#if(n.di < 2*((1/2)^6)) {"OP min p > 0.125."}

#within.di.samp <- sample(within.di, size = n.di, replace = FALSE)
within.di.mean <- mean(within.di)

#across.di.samp <- sample(across.di, size = n.di, replace = FALSE)
across.di.mean <- mean(across.di)

#(wt.DI <- wilcox.test(within.di.samp,across.di.samp, conf.int = TRUE, conf.level = 0.95, paired = TRUE))

normed.mu <- within.di.mean/across.di.mean

subj_table <- rbind(subj_table, data.frame(subject = curr.subj,
                                           test = 'duplets',
                                           n.with = length(within.di),
                                           n.accr = length(across.di),
                                           mean.with = within.di.mean,
                                           mean.accr = across.di.mean,
                                           score = normed.mu))
                                           # w = wt.DI$statistic,
                                           # CI.low = wt.DI$conf.int[1],
                                           # CI.hi = wt.DI$conf.int[2],
                                           # min_p = 2*((1/2)^n.di),
                                           # p = wt.DI$p.value))

} # subject

subj_table <- subj_table[-1,]
write_csv(subj_table,file.path(res_path,'wilcoxon_subjects_final.csv'))
```
"Normed.mu", or "score", stored in the subj_table matrix is simply the ratio between within and across means. However, this value turned out not to be very useful as a metric, so we did not end up using it. 


## Load WordRec
```{r}
if (all(count(subj_table$subject)$freq==4) & last(subj_table$subject)==33) {subj_table$subject <- rep(unique(exp.3.only$subject),each=4)}

# Read in file if starting here. 
subj_table <- read_csv(file.path(res_path,'wilcoxon_subjects_final.csv')) %>%
              mutate(subject = as.factor(subject),
                    test = as.factor(test))


data_path <- 'C:/Users/Ava/Desktop/Experiments/statistical_learning/1_data/exp_3'
wordrec_data_orig <- read_csv(file.path(data_path,'wordrec_task_data.csv')) %>%
    dplyr::select(subj_id:zabetu) %>%
    mutate(subject = as.factor(subj_id), subj_id = NULL) %>% 
    dplyr::select(subject,prop_word_chosen:zabetu)


wordrec_data_corr <- wordrec_data_orig[(wordrec_data_orig$subject) %in% unique(subj_table$subject),] 
subj_table_corr <- subj_table[(subj_table$subject) %in% unique(wordrec_data_corr$subject),] 
length(unique(subj_table_corr$subject))
length(unique(wordrec_data_corr$subject))

corr.table <- subj_table_corr %>% add_column(prop.word = rep(wordrec_data_corr$prop_word_chosen,each = 4))
```

### Create Score
```{r}
corr.table$sub <- corr.table$mean.with-corr.table$mean.accr
summary(corr.table$sub)
corr.table$sub.scale <- scale(corr.table$mean.with-corr.table$mean.accr, center = 0)
summary(corr.table$sub.scale)
corr.table <- corr.table %>% mutate(sub.scale.or = sub.scale)
corr.table$sub.scale.or[which(corr.table$sub.scale.or>1 | corr.table$sub.scale.or < -1)] <- NA
summary(corr.table$sub.scale.or)
corr.table <- corr.table %>%mutate(log.sub = log(sub.scale + 3))
summary(corr.table$log.sub)
corr.table <- corr.table %>%mutate(log.sub.or = log.sub)
corr.table$log.sub.or[which(corr.table$log.sub.or>1 | corr.table$log.sub.or < -1)] <- NA 
summary(corr.table$log.sub.or)
```

### -> Correlate
```{r}
(corr.ordp <- cor.test(corr.table$sub[corr.table$test=="ordinal position"],
                     corr.table$prop.word[corr.table$test=="ordinal position"], 
                     method = "pearson",
                     alternative = "greater"))


(corr.prob <- cor.test(corr.table$sub[corr.table$test=="transitional probability"],
                     corr.table$prop.word[corr.table$test=="transitional probability"], 
                     method = "pearson",
                     alternative = "greater"))

(corr.word <- cor.test(corr.table$sub[corr.table$test=="word identity"],
                     corr.table$prop.word[corr.table$test=="word identity"], 
                     method = "pearson",
                     alternative = "greater"))

(corr.dups <- cor.test(corr.table$sub[corr.table$test=="duplets"],
                     corr.table$prop.word[corr.table$test=="duplets"], 
                     method = "pearson",
                     alternative = "greater"))

```

### -> Plot
```{r warning=FALSE}
corr.table$test <- factor(corr.table$test,levels = c("ordinal position","transitional probability","word identity","duplets"))

plots.text <- data.frame(test = factor(c("ordinal position","transitional probability","word identity","duplets"),
                                       levels = c("ordinal position","transitional probability","word identity","duplets")),
  label = c(paste0("\u03C1 = ",round(corr.ordp$estimate, digits = 2),", p = ",round(corr.ordp$p.value, digits = 2)),
             paste0("\u03C1 = ",round(corr.prob$estimate, digits = 2),", p = ",round(corr.prob$p.value, digits = 2)),
             paste0("\u03C1 = ",round(corr.word$estimate, digits = 2),", p = ",round(corr.word$p.value, digits = 2)),
             paste0("\u03C1 = ",round(corr.dups$estimate, digits = 2),", p = ",round(corr.dups$p.value, digits = 2))))
```


```{r warning=FALSE}
# Let's do the whole thing! 
ggplot(corr.table, aes(sub, prop.word, color = test)) + 
  geom_point(size = 2, alpha = 0.8) +
  geom_smooth(method = lm, se = FALSE, size = 1.1) +
 scale_color_manual(values = c("#D95C4E", "#E8B651", "#7BB095", "#5E8FEB"),
                     name = NULL, 
                     guide = NULL) +
  scale_x_continuous(name="within-across similarity z(Pearson's r)") + 
  scale_y_continuous(name="% correct (out of 16)") +
# annotate(geom = "text", x = -0.6, y = 1.1, label = paste0("\u03C1 = ",round(corr.ordp$estimate, digits = 2),", p = ",round(corr.ordp$p.value, digits = 2)),
#         size = 4, color = "#D95C4E") +
# annotate(geom = "text", x = -0.22, y = 1.1, label = paste0("\u03C1 = ",round(corr.prob$estimate, digits = 2),", p = ",round(corr.prob$p.value, digits = 2)),
#         size = 4, color = "#E8B651", fontface = "bold") +
# annotate(geom = "text", x = 0.22, y = 1.1, label = paste0("\u03C1 = ",round(corr.word$estimate, digits = 2),", p = ",round(corr.word$p.value, digits = 2)),
#         size = 4, color = "#7BB095") +
# annotate(geom = "text", x = 0.6, y = 1.1, label = paste0("\u03C1 = ",round(corr.dups$estimate, digits = 2),", p = ",round(corr.dups$p.value, digits = 2)),
#         size = 4, color = "#5E8FEB") +
#---------------------------------------- if facets, plot on each plot the stats
  geom_text(data = plots.text, mapping = aes(x = -Inf, y = -Inf, label = label), color = "black", hjust = -0.1, vjust = -1, size = 5) +
  facet_wrap(. ~ test,nrow = 2)+ 
  geom_vline(xintercept = 0, color = "gray", linetype="dashed") + 
  theme_classic() +
  theme(text = element_text(family = "LM Roman 10", face="bold",size=20), 
        legend.title = element_text(""),  legend.position = "top") +
  ggsave(file.path(fig_path,'fig10_corr_sub_final_labels_x_facet2_col.png'),width = 9, height = 7)  



```

