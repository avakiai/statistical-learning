---
title: "Study3-4_RTCorrelationPatternAnalysis"
author: "Ava Kiai"
date: "9/8/2020"
output: html_document
---
Code for the final two analyses in Kiai & Melloni 2020: RSA on group-level RTs for each feature, and RSA for individuals, correlated with their word recognition performance. 

## Load & Init.
```{r}
library(tidyverse)
library(extrafont)
par(family="LM Roman 10")
library(DescTools) 
```

#Path
```{r}
# Relative Paths
# Code Path
code_path <- getwd()
# Data Path
data_path <- file.path(code_path,'../../', '1_data', 'combined')
# Results Path
res_path <- file.path(code_path,'../../','3_results','combined')
# Final fig path
fig_path <-  file.path(code_path,'../../','3_results','final')

exps.3.4.data <- read_csv(file.path(data_path, 'exps_3_4s_data.csv'), col_types = "cccdddcddddc") %>% mutate(subject = as.factor(subject),
                  cond_order = as.character(cond_order),
                  sess = as.character(sess), 
                  target = as.factor(target),
                  tgt_pos = as.factor(tgt_pos), 
                  tgt_word = as.factor(tgt_word),
                  exp = as.factor(exp))

exps.3.4.data <- exps.3.4.data %>%
  mutate(target = factor(target, levels = c("nu","ga","di","ro","ki","se","mi","po","la","za","be","tu")))

```

## Compute Correlation Matrices
```{r}
# For each subject, compute a correlation matrix between syllables and then z transform that data. Put matrices into a list, and average them. Then transform back to rho. 
# We wanted to extract several layers of information here: 
  all.list  <- list() # each sub's list of RTs
  all.cor.list  <- list() # corr matrixes
  all.cor.z.list  <- list() # z-trans corr mat
  all.cor.diss.z.list <- list() # z-trans dissimilarity matrices (1-corr)
  all.cor.diss.list <- list() # diss matrices (1-corr)

for (curr_subj in unique(exps.3.4.data$subject)) {
  curr_data <- exps.3.4.data %>% 
    filter(subject == curr_subj) %>%
    dplyr::select(target,rt) %>% 
    na.omit() %>%
    group_by(target) %>%
    dplyr::mutate(row_id=1:n()) %>%
    ungroup() %>%  
    spread(key = target, value = rt) %>%
    #ungroup(subject)
    dplyr::select(-row_id) 
  
# find minimum number of rows, N
  n_rows <- curr_data %>% summarise_all(funs(sum(!is.na(.)))) %>% unlist() %>% unname()
# exceptions: some participants had imbalances in the data that need to be fixed for the analysis, e.g. exceptionally few values for a syllable - in this case, remove it so that the field is NA and in the final analysis, it is just skipped
  if(curr_subj == "r4893") { # 3/29
    curr_data <- curr_data %>% 
    mutate(tu = as.numeric(rep(NA,nrow(curr_data))))
    N <- sort(n_rows)[2] # select the second lowest length
  } else if(curr_subj == "l6988") { # 3/26
    # subject had only 2 entires in tu, so just make the column NA to preserve other data
    curr_data <- curr_data %>% 
    mutate(tu = as.numeric(rep(NA,nrow(curr_data))))
    N <- sort(n_rows)[2] # select the second lowest length
  } else if (curr_subj == "i3939") { # 3/21
    curr_data <- curr_data %>% 
    mutate(be = as.numeric(rep(NA,nrow(curr_data))))
    N <- sort(n_rows)[2] 
  } else if (curr_subj == "v7564") { # 3/11
    curr_data <- curr_data %>% 
    add_column(ro = as.numeric(rep(NA,nrow(curr_data))))
    n_rows <- curr_data %>% summarise_all(funs(sum(!is.na(.)))) %>% unlist() %>% unname()
    N <- sort(n_rows)[2]
  } else if (curr_subj == "s5965") { # 3/8
    curr_data <- curr_data %>% 
    mutate(po = as.numeric(rep(NA,nrow(curr_data))))
    N <- sort(n_rows)[2] # select the second lowest length
    # and maybe also ga? 
  } else if (curr_subj == "s7540") { # 3/2
    curr_data <- curr_data %>% 
    add_column(be = as.numeric(rep(NA,nrow(curr_data))),.after = "za") %>%
    add_column(ro = as.numeric(rep(NA,nrow(curr_data))),.after = "di")
    n_rows <- curr_data %>% summarise_all(funs(sum(!is.na(.)))) %>% unlist() %>% unname()
    N <- sort(n_rows)[3]
  } else if (curr_subj == "m0085") { # 3/1
    curr_data <- curr_data %>% 
    mutate(be = as.numeric(rep(NA,nrow(curr_data))))
    N <- sort(n_rows)[2] 
  } else if (curr_subj == "h4610") { # 4/16    
    curr_data <- curr_data %>% 
    add_column(be = as.numeric(rep(NA,nrow(curr_data))),.after = "za") %>%
    add_column(se = as.numeric(rep(NA,nrow(curr_data))),.after = "ki") %>%
    mutate(za = as.numeric(rep(NA,nrow(curr_data)))) %>%
    mutate(ga = as.numeric(rep(NA,nrow(curr_data))))
    n_rows <- curr_data %>% summarise_all(funs(sum(!is.na(.)))) %>% unlist() %>% unname()
    N <- sort(n_rows)[5]
  } else if (curr_subj == "x5767") { # 4/12
    # subject is missing be data, so just make it NA, N is the same
    curr_data <- curr_data %>% 
    add_column(be = as.numeric(rep(NA,nrow(curr_data))),.after = "za")
    n_rows <- curr_data %>% summarise_all(funs(sum(!is.na(.)))) %>% unlist() %>% unname()
    N <- sort(n_rows)[2]
  } else if (curr_subj == "h9453") { # 4/2
    curr_data <- curr_data %>% 
    mutate(be = as.numeric(rep(NA,nrow(curr_data)))) %>%
    mutate(tu = as.numeric(rep(NA,nrow(curr_data))))
    N <- sort(n_rows)[3]
  } else {
    N <- min(n_rows)
  }
# create output matrix with N rows
  curr_data.mat <- curr_data[1:N,]
# shuffle longer columns into shorter new matrix
  for (i in 1:length(curr_data.mat)) {
  if (n_rows[i] < N) {curr_data.mat[,i] <- sample(as.matrix(curr_data[1:N,i]), size = N)
  } else {curr_data.mat[,i] <- sample(as.matrix(curr_data[1:n_rows[i],i]), size = N)}
  }

  
# take correlation(), then transform to z values()
  curr_data.cor <- cor(curr_data.mat) # corr
  curr_data.cor.z <- FisherZ(cor(curr_data.mat)) # corr - fisher
  curr_data.cor.diss.z <- FisherZ(1-cor(curr_data.mat)) # diss - fisher
  curr_data.cor.diss <- 1-(cor(curr_data.mat)) # diss

# append
  all.list <- append(all.list, list(curr_data.mat)) # Regular matrices, uneven
  all.cor.list <- append(all.cor.list, list(curr_data.cor)) # Correlations
  all.cor.z.list <- append(all.cor.z.list, list(curr_data.cor.z)) # Z-transformed correlations
  all.cor.diss.z.list <- append(all.cor.diss.z.list, list(curr_data.cor.diss.z)) # Z-transformed 1-correlations
  all.cor.diss.list <- append(all.cor.diss.list, list(curr_data.cor.diss)) # 1-correlations
}


# Fisher Z merely makes the sampling distribution more normal to make calculation easier, but
# it's not required if the computation is performed on a computer that can handle the exact 
# (however skewed) true distribution: https://stats.stackexchange.com/questions/420142/why-is-fisher-transformation-necessary

# For the z's, the diag is infinite
# all.cor <- apply(simplify2array(all.cor.list), 1:2, mean, na.rm= TRUE)
# all.z <- apply(simplify2array(all.cor.z.list), 1:2, mean, na.rm= TRUE)
# The results of these two are nearly identical -- identical out to about 3 digits, but indeed the z's are normal-ish

# The problem with all.cor.diss.z.list is that some 1-corr values are > 1, which is incalculable for the z transformation. Thus, many
# values become NA. 
  
  
(all.cor.z.list)  
```
To avoid data loss due to particularities of the Fisher Z transformation, and to normalize the distribution of correlation values, we will perform the RSA on z-transformed similarity (corrlation) values. 

# Group-Level Analysis
Procedure: 
1. Take 1-correlations for each subject, z transform
#### Set analysis type
```{r}
# Group level analysis on this matrix: 

# Dissimilarity Matrix
# group.analysis <- all.cor.diss.list
# name <- 'diss'
# label <- 'dissimilarity (1-'

# group.analysis <- all.cor.diss.z.list
# name <- 'diss_z'
# label <- 'dissimilarity z(1-'

# Similarity Matrix
# group.analysis <- all.cor.list
# name <- 'corr'
# label <- 'similarity ('

group.analysis <- all.cor.z.list
name <- 'corr_z'
label <- 'similarity z('

```

2. Sample 200 times with replacement from all participant's matrices for each within vs. across category; subsample arrays to make them equal in length.
3. Compute wilcoxon

For each test, we look at the correlation between items in each category vs. items outside that category.


#### Set Seed
```{r}
set.seed(42)
```

### - Ordinal Position
Within: 
1s: (all ones vs. all other ones) nu vs. ro, nu vs. mi, etc.  
2s: (all 2s vs. all other twos) ga vs. ki, etc. 
3s: (all threes vs. all other threes) di vs. se, etc. 
... together 

Across:
1-2s, 1-3s, 2-3s: all crossed positions nu vs. ga, ga vs. di, nu vs. diy

nu 1 ga 2 di 3
ro 4 ki 5 se 6
mi 7 po 8 la 9
za 10 be 11 tu 12
```{r}

# indices for each matrix
wn.OP <- list(c(1,4),c(1,7),c(1,10),c(4,7),c(4,10),c(7,10), # 1s
             c(2,5),c(2,8),c(2,11),c(5,8),c(5,11),c(8,11), #2s
             c(3,6),c(3,9),c(3,12),c(6,9),c(6,12),c(9,12)) #3s
ac.OP <- list(c(1,2),c(2,3),c(1,3),
              c(4,5),c(5,6),c(4,6), 
             c(7,8),c(7,9),c(8,9),
             c(10,11),c(11,12),c(10,12),
             c(1,5),c(1,6),c(1,8),c(1,9),c(1,11),c(1,12),
             c(2,4),c(3,4),c(4,8),c(4,9),c(4,11),c(4,12),
             c(2,7),c(3,7),c(5,7),c(6,7),c(7,11),c(7,12),
             c(2,10),c(3,10),c(5,10),c(6,10),c(8,10),c(9,10),
             c(2,6),c(2,9),c(2,12),
             c(3,5),c(5,9),c(5,12),
             c(3,8),c(6,8),c(8,12),
             c(3,11),c(6,11),c(9,11)) 

# collect values for each of these boxes from all participants (all matrices in the list)
# You can check what's happening here like so: group.analysis[[1]][wn.OP[[i]][1],wn.OP[[i]][2]]
wn.arr <- list()
for (i in 1:length(wn.OP)) {wn.arr <- append(wn.arr, mapply(function(x, y) x[y][2], group.analysis, wn.OP[i], SIMPLIFY = FALSE))} 
ac.arr <- list()
for (i in 1:length(ac.OP)) {ac.arr <- append(ac.arr, mapply(function(x, y) x[y][2], group.analysis, ac.OP[i], SIMPLIFY = FALSE))} 

within.OP <- cbind(wn.arr) %>% as.numeric()
within.OP[mapply(is.infinite, within.OP)] <- NA # clean, in case using z-transform
loss <- length(within.OP[is.na(within.OP)])/length(within.OP)
if (loss > 0.1) {stop("Within OP loss too high.")}
within.OP <- within.OP[!is.na(within.OP)] # remove NA

across.OP <- cbind(ac.arr) %>% as.numeric()
across.OP[mapply(is.infinite, across.OP)] <- NA
loss <- length(across.OP[is.na(across.OP)])/length(across.OP) 
if (loss > 0.1) {stop("Across OP loss too high.")}
across.OP <- across.OP[!is.na(across.OP)] # remove NA

# compute maximum length
(n.op <- round(min(length(within.OP),length(across.OP))*(4/5)))

# sample & bootstrap to produce a matrix and bootstrapped means
set.seed(42)
within.OP.samp <- as.numeric(replicate(200, sample(within.OP, size = n.op, replace = TRUE)))
set.seed(42)
within.OP.medians <- replicate(200, median(sample(within.OP, size = n.op, replace = TRUE), na.rm = TRUE)) 
set.seed(42)
across.OP.samp <- as.numeric(replicate(200, sample(across.OP, size = n.op, replace = TRUE)))
set.seed(42)
across.OP.medians <- replicate(200, median(sample(across.OP, size = n.op, replace = TRUE), na.rm = TRUE)) 


(wt.OP <- wilcox.test(within.OP.samp,across.OP.samp, conf.int = TRUE, conf.level = 0.95, paired = TRUE))
```


### - Transitional Probability
Within:
1's: all syllables with TP = 1


Across: 
.33's: all syllables with TP = 0.33

nu 1 ga 2 di 3
ro 4 ki 5 se 6
mi 7 po 8 la 9
za 10 be 11 tu 12
```{r}

wn.TP <- list(c(2,3),c(5,6),c(8,9),c(11,12),
              c(2,6),c(2,9),c(2,12),
             c(3,5),c(5,9),c(5,12),
             c(3,8),c(6,8),c(8,12),
             c(3,11),c(6,11),c(9,11),
             c(2,5),c(2,8),c(2,11),c(5,8),c(5,11),c(8,11), #2s
             c(3,6),c(3,9),c(3,12),c(6,9),c(6,12),c(9,12)) #3s # high TP
ac.TP <- list(c(1,4),c(1,7),c(1,10),c(4,7),c(4,10),c(7,10)) # low TP

wn.tp.arr <- list()
for (i in 1:length(wn.TP)) {
    wn.tp.arr <- append(wn.tp.arr, mapply(function(x, y) x[y][2], group.analysis, wn.TP[i], SIMPLIFY = FALSE))
} 

ac.tp.arr <- list()
for (i in 1:length(ac.TP)) {
    ac.tp.arr <- append(ac.tp.arr, mapply(function(x, y) x[y][2], group.analysis, ac.TP[i], SIMPLIFY = FALSE))
} 

within.TP <- cbind(wn.tp.arr) %>% as.numeric()
within.TP[mapply(is.infinite, within.TP)] <- NA
loss <- length(within.TP[is.na(within.TP)])/length(within.TP)
if (loss > 0.1) {print(paste0("Within TP loss: ",round(loss, digits = 3),"%."))
  #stop("Within TP loss too high.")
  }
within.TP <- within.TP[!is.na(within.TP)] # remove NA


across.TP <- cbind(ac.tp.arr) %>% as.numeric()
across.TP[mapply(is.infinite, across.TP)] <- NA
loss <- length(across.TP[is.na(across.TP)])/length(across.TP)
if (loss > 0.1) {print(paste0("Across TP loss: ",round(loss, digits = 3),"%."))
  #stop("Across TP loss too high.")
  }
across.TP <- across.TP[!is.na(across.TP)] # remove NA

(n.tp <- round(min(length(within.TP),length(across.TP))*(4/5)))

set.seed(42)
within.TP.samp <- replicate(200, sample(within.TP, size = n.tp, replace = TRUE)) 
set.seed(42)
within.TP.medians <- replicate(200, median(sample(within.TP, size = n.tp, replace = TRUE), na.rm = TRUE)) 
set.seed(42)
across.TP.samp <- replicate(200, sample(across.TP, size = n.tp, replace = TRUE)) 
set.seed(42)
across.TP.medians <- replicate(200, median(sample(across.TP, size = n.tp, replace = TRUE), na.rm = TRUE)) 


(wt.TP <- wilcox.test(within.TP.samp,across.TP.samp, conf.int = TRUE, conf.level = 0.95, paired = TRUE))

```

### - Triplet Identity
Within: all comparisons within a words of the form 1-2, 2-3, 1-3
nu-ga, ga-di, nu-di
across: all "phantom word" comparisons of the form 1-2,2-3,1-3,

nu 1 ga 2 di 3
ro 4 ki 5 se 6
mi 7 po 8 la 9
za 10 be 11 tu 12
```{r}

wn.WI <- list(c(1,2),c(2,3),c(1,3),# nugadi
              c(4,5),c(5,6),c(4,6),# rokise
             c(7,8),c(7,9),c(8,9),# mipola
             c(10,11),c(11,12),c(10,12))# zabetu
ac.WI <- list(c(1,5),c(1,6),c(1,8),c(1,9),c(1,11),c(1,12),
             c(2,4),c(3,4),c(4,8),c(4,9),c(4,11),c(4,12),
             c(2,7),c(3,7),c(5,7),c(6,7),c(7,11),c(7,12),
             c(2,10),c(3,10),c(5,10),c(6,10),c(8,10),c(9,10),
              c(2,6),c(2,9),c(2,12),
             c(3,5),c(5,9),c(5,12),
             c(3,8),c(6,8),c(8,12),
             c(3,11),c(6,11),c(9,11))

wn.WI.arr <- list()
for (i in 1:length(wn.WI)) {
    wn.WI.arr <- append(wn.WI.arr, mapply(function(x, y) x[y][2], group.analysis, wn.WI[i], SIMPLIFY = FALSE))
} 

ac.WI.arr <- list()
for (i in 1:length(ac.WI)) {
    ac.WI.arr <- append(ac.WI.arr, mapply(function(x, y) x[y][2], group.analysis, ac.WI[i], SIMPLIFY = FALSE))
} 

within.WI <- cbind(wn.WI.arr) %>% as.numeric()
within.WI[mapply(is.infinite, within.WI)] <- NA
loss <- length(within.WI[is.na(within.WI)])/length(within.WI)
if (loss > 0.1) {print(paste0("Within WI loss: ",round(loss, digits = 3),"%."))
  #stop("Within WI loss too high.")
  }
within.WI <- within.WI[!is.na(within.WI)] # remove NA

across.WI <- cbind(ac.WI.arr) %>% as.numeric()
across.WI[mapply(is.infinite, across.WI)] <- NA
loss <- length(across.WI[is.na(across.WI)])/length(across.WI)
if (loss > 0.1) {print(paste0("Across WI loss: ",round(loss, digits = 3),"%."))
 # stop("Across WI loss too high.")
  }
across.WI <- across.WI[!is.na(across.WI)] # remove NA

(n.wi <- round(min(length(within.WI),length(across.WI))*(4/5)))

set.seed(42)
within.WI.samp <- replicate(200, sample(within.WI, size = n.wi, replace = TRUE)) 
set.seed(42)
within.WI.medians <- replicate(200, median(sample(within.WI, size = n.wi, replace = TRUE), na.rm = TRUE)) 
set.seed(42)
across.WI.samp <- replicate(200, sample(across.WI, size = n.wi, replace = TRUE)) 
set.seed(42)
across.WI.medians <- replicate(200, median(sample(across.WI, size = n.wi, replace = TRUE), na.rm = TRUE)) 

(wt.WI <- wilcox.test(within.WI.samp,across.WI.samp, conf.int = TRUE, conf.level = 0.95, paired = TRUE))
```


### - Duplet Identity
Within: All proper duplets, 1-2s, 2-3s
Across: 3-1's

nu 1 ga 2 di 3
ro 4 ki 5 se 6
mi 7 po 8 la 9
za 10 be 11 tu 12
```{r}
set.seed(42)

wn.di <- list(c(1,2),c(2,3),
              c(4,5),c(5,6),
             c(7,8),c(8,9),
             c(10,11),c(11,12))
ac.di <- list(c(3,4),c(3,7),c(3,10),
              c(1,6),c(6,7),c(6,10),
              c(1,9),c(4,9),c(9,10),
              c(1,12),c(4,12),c(7,12))


wn.di.arr <- list()
for (i in 1:length(wn.di)) {
    wn.di.arr <- append(wn.di.arr, mapply(function(x, y) x[y][2], group.analysis, wn.di[i], SIMPLIFY = FALSE))
} 

ac.di.arr <- list()
for (i in 1:length(ac.di)) {
    ac.di.arr <- append(ac.di.arr, mapply(function(x, y) x[y][2], group.analysis, ac.di[i], SIMPLIFY = FALSE))
} 

within.di <- cbind(wn.di.arr) %>% as.numeric()
within.di[mapply(is.infinite, within.di)] <- NA
loss <- length(within.di[is.na(within.di)])/length(within.di)
if (loss > 0.1) {print(paste0("Within DI loss: ",round(loss, digits = 3),"%."))
 # stop("Within DI loss too high.")
  }
within.di <- within.di[!is.na(within.di)] # remove NA

across.di <- cbind(ac.di.arr) %>% as.numeric()
across.di[mapply(is.infinite, across.di)] <- NA
loss <- length(across.di[is.na(across.di)])/length(across.di)
if (loss > 0.1) {print(paste0("Across DI loss: ",round(loss, digits = 3),"%."))
  #stop("Across DI loss too high.")
  }
across.di <- across.di[!is.na(across.di)] # remove NA

(n.di <- round(min(length(within.di),length(across.di))*(4/5)))

set.seed(42)
within.di.samp <- replicate(200, sample(within.di, size = n.di, replace = TRUE))
set.seed(42)
within.di.medians <- replicate(200, median(sample(within.di, size = n.di, replace = TRUE), na.rm = TRUE))
set.seed(42)
across.di.samp <- replicate(200, sample(across.di, size = n.di, replace = TRUE))
set.seed(42)
across.di.medians <- replicate(200, median(sample(across.di, size = n.di, replace = TRUE), na.rm = TRUE))

(wt.DI <- wilcox.test(within.di.samp,across.di.samp, conf.int = TRUE, conf.level = 0.95, paired = TRUE))

```

This was essentially 4 Mann-Whitney Tests:  if both x and y are given and paired is FALSE, a Wilcoxon rank sum test (equivalent to the Mann-Whitney test: see the Note) is carried out. In this case, the null hypothesis is that the distributions of x and y differ by a location shift of mu and the alternative is that they differ by some other location shift (and the one-sided alternative "greater" is that x is shifted to the right of y).

P-values computed via normal approximation.

Note that in the two-sample case the estimator for the difference in location parameters does not estimate the difference in medians (a common misconception) but rather the median of the difference between a sample from x and a sample from y.

### -> Combine Data/Wilcoxon
```{r}

WA.wilcox <- data.frame(test = factor(c(
"triplet position",
"transitional probability",
"word grouping",
"duplet pairing"), levels = c("triplet position","transitional probability","word grouping","duplet pairing")),
median_diff = as.numeric(c(median(within.OP.medians)-median(across.OP.medians),
median(within.TP.medians)-median(across.TP.medians),
median(within.WI.medians)-median(across.WI.medians),
median(within.di.medians)-median(across.di.medians))),
estimate = as.numeric(c(
wt.OP$estimate,
wt.TP$estimate,
wt.WI$estimate,
wt.DI$estimate)),
N = as.numeric(c(
n.op,
n.tp,
n.wi,
n.di)),
w = as.numeric(c(
wt.OP$statistic,
wt.TP$statistic,
wt.WI$statistic,
wt.DI$statistic)),
"CI low" = as.numeric(c(
wt.OP$conf.int[1],
wt.TP$conf.int[1],
wt.WI$conf.int[1],
wt.DI$conf.int[1])),
"CI hi" = as.numeric(c(
wt.OP$conf.int[2],
wt.TP$conf.int[2],
wt.WI$conf.int[2],
wt.DI$conf.int[2])),
p = as.numeric(c(
wt.OP$p.value,
wt.TP$p.value,
wt.WI$p.value,
wt.DI$p.value)))



# Bonferroni correction
WA.wilcox$p.adj <- p.adjust(WA.wilcox$p, method = "bonferroni", n = length(WA.wilcox$p))


write.csv(WA.wilcox,file.path(res_path,paste0('/wilcox_',name,'.csv')), row.names = FALSE)

```

### -> Plot
```{r}
WA.wilcox <- read_csv(file.path(res_path,paste0('/wilcox_',name,'.csv'))) %>%
  mutate(test = factor(test,levels = c("triplet position","transitional probability","word grouping","duplet pairing")))
```
Graphical representation
```{r}
syllables <- c("nu","ga","di","ro","ki","se","mi","po","la","za","be","tu")
triplet <- rbind(
  data.frame(group = factor(rep("within",length(wn.OP)),levels = c("within","across")),
             x = sapply(wn.OP, "[[", 1), y = sapply(wn.OP, "[[", 2)),
  data.frame(group = factor(rep("across",length(ac.OP)),levels = c("within","across")),
             x = sapply(ac.OP, "[[", 1), y = sapply(ac.OP, "[[", 2))
  )
triplet <- triplet %>%
  mutate(xn = factor(syllables[triplet$x],levels = c("nu","ga","di","ro","ki","se","mi","po","la","za","be","tu")),
        yn = factor(syllables[triplet$y],c("nu","ga","di","ro","ki","se","mi","po","la","za","be","tu")))

trans <- rbind(
  data.frame(group = factor(rep("within",length(wn.TP)),levels = c("within","across")),
                            x = sapply(wn.TP, "[[", 1), y = sapply(wn.TP, "[[", 2)),
  data.frame(group = factor(rep("across",length(ac.TP)),levels = c("within","across")),
                            x = sapply(ac.TP, "[[", 1), y = sapply(ac.TP, "[[", 2))
  )
trans <- trans %>%
  mutate(xn = factor(syllables[trans$x],c("nu","ga","di","ro","ki","se","mi","po","la","za","be","tu")),
         yn = factor(syllables[trans$y],c("nu","ga","di","ro","ki","se","mi","po","la","za","be","tu")))

word <- rbind(
  data.frame(group = factor(rep("within",length(wn.WI)),levels = c("within","across")),
             x = sapply(wn.WI, "[[", 1), y = sapply(wn.WI, "[[", 2)),
  data.frame(group = factor(rep("across",length(ac.WI)),levels = c("within","across")),
             x = sapply(ac.WI, "[[", 1), y = sapply(ac.WI, "[[", 2))
  )
word <- word %>%
  mutate(xn = factor(syllables[word$x],c("nu","ga","di","ro","ki","se","mi","po","la","za","be","tu")),
         yn = factor(syllables[word$y],c("nu","ga","di","ro","ki","se","mi","po","la","za","be","tu")))

duplet <- rbind(
  data.frame(group = factor(rep("within",length(wn.di)),levels = c("within","across")),
             x = sapply(wn.di, "[[", 1), y = sapply(wn.di, "[[", 2)),
  data.frame(group = factor(rep("across",length(ac.di)),levels = c("within","across")),
             x = sapply(ac.di, "[[", 1), y = sapply(ac.di, "[[", 2))
  )
duplet <- duplet %>%
  mutate(xn = factor(syllables[duplet$x],c("nu","ga","di","ro","ki","se","mi","po","la","za","be","tu")),
        yn = factor(syllables[duplet$y],c("nu","ga","di","ro","ki","se","mi","po","la","za","be","tu")))



```

Triplet Position
```{r}
ggplot(triplet, aes(xn,yn,fill = group)) + 
  geom_tile() + 
  scale_fill_manual(values = c("#0d0d0d","#a6a6a6"), name = NULL) + 
  scale_x_discrete(limits = c("nu","ga","di","ro","ki","se","mi","po","la","za","be","tu"),
                  breaks=c("nu","ga","di","ro","ki","se","mi","po","la","za","be","tu"),
                   position = "top", name = NULL) +
  scale_y_discrete(breaks=c("nu","ga","di","ro","ki","se","mi","po","la","za","be","tu"),
                   limits=rev(c("nu","ga","di","ro","ki","se","mi","po","la","za","be","tu")), name = NULL) +
  theme_classic() + 
  theme(text = element_text(family = "LM Roman 10", face="bold", size = 15))

```
Transitional Probability
```{r}
ggplot(trans, aes(xn,yn,fill = group)) + 
  geom_tile() + 
  scale_fill_manual(values = c("#0d0d0d","#a6a6a6"), name = NULL) + 
  scale_x_discrete(limits = c("nu","ga","di","ro","ki","se","mi","po","la","za","be","tu"),
                   breaks=c("nu","ga","di","ro","ki","se","mi","po","la","za","be","tu"),
                   position = "top", name = NULL) +
  scale_y_discrete(limits = rev(c("nu","ga","di","ro","ki","se","mi","po","la","za","be","tu")),
                   breaks=c("nu","ga","di","ro","ki","se","mi","po","la","za","be","tu"),
                   name = NULL) +
  theme_classic() + 
  theme(text = element_text(family = "LM Roman 10", face="bold", size = 15))
```

Word Grouping
```{r}
ggplot(word, aes(xn,yn,fill = group)) + 
  geom_tile() + 
  scale_fill_manual(values = c("#0d0d0d","#a6a6a6"), name = NULL) + 
  scale_x_discrete(limits = c("nu","ga","di","ro","ki","se","mi","po","la","za","be","tu"),
                  breaks=c("nu","ga","di","ro","ki","se","mi","po","la","za","be","tu"),
                   position = "top", name = NULL) +
  scale_y_discrete(breaks=c("nu","ga","di","ro","ki","se","mi","po","la","za","be","tu"),
                   limits=rev(c("nu","ga","di","ro","ki","se","mi","po","la","za","be","tu")), name = NULL) +
  theme_classic() + 
  theme(text = element_text(family = "LM Roman 10", face="bold", size = 15))
```
Duplet 
```{r}
ggplot(duplet, aes(xn,yn,fill = group)) + 
  geom_tile() + 
  scale_fill_manual(values = c("#0d0d0d","#a6a6a6"), name = NULL) + 
  scale_x_discrete(limits = c("nu","ga","di","ro","ki","se","mi","po","la","za","be","tu"),
                  breaks=c("nu","ga","di","ro","ki","se","mi","po","la","za","be","tu"),
                   position = "top", name = NULL) +
  scale_y_discrete(breaks=c("nu","ga","di","ro","ki","se","mi","po","la","za","be","tu"),
                   limits=rev(c("nu","ga","di","ro","ki","se","mi","po","la","za","be","tu")), name = NULL) +
  theme_classic() + 
  theme(text = element_text(family = "LM Roman 10", face="bold", size = 15))
```

```{r}
ggplot(WA.wilcox) + 
  geom_col(aes(test,median_diff,fill=test), color = "black") + #, fill = "grey", color = "black") + 
  geom_hline(yintercept = 0, color = "black", linetype="dashed") + 
  scale_x_discrete(name = NULL) + 
  scale_y_continuous(name = paste0("within - across ", label ,"Pearson's r)")) +
  scale_fill_manual(values = c("#D95C4E", "#E8B651", "#7BB095", "#5E8FEB"), guide = NULL) +
  #annotate(geom = "text", x = 1, y = 0.052, size = 4, label = "***") +
  #annotate(geom = "text", x = 2, y = 0.09, size = 4, label = "***") +
  #annotate(geom = "text", x = 4, y = 0.058, size = 4, label = "***") +
  # annotate(geom = "text", x = 1, y = 0.055, size = 4, label = "p<0.0001\n***") +
  # annotate(geom = "text", x = 2, y = 0.094, size = 4, label = "p<0.0001\n***") +
  # annotate(geom = "text", x = 4, y = 0.062, size = 4, label = "p<0.0001\n***") +
  theme_classic() + 
  theme(text = element_text(family = "LM Roman 10", face="bold", size = 15)) #+
  ggsave(file.path(fig_path, paste0('/fig4_',name,'.png')), width = 9, height = 5)  

```



# Subject-Level Analysis

### Recompute Correlation Matrices for Experiment 3 only
The subjects used in the previous analysis were from both exp's 3 and 4. Here, we want only those from experiment 3, so that we can compare their rt patterns with their offline word recognition performance. We'll re-run the analysis above but separate out the kids from the first experiment first. 
```{r eval=FALSE, warning=FALSE, include=FALSE}
# For each subject, compute a correlation matrix between syllables and then z transform that data. Put matrices into a list, and average them. Then transform back to rho. 
  exp3.list  <- list()
  exp3.cor.list  <- list()
  exp3.cor.z.list  <- list()
  exp3.cor.diss.z.list <- list()
  exp3.cor.diss.list <- list()

exp.3.only <- exps.3.4.data %>% filter(exp == "exp 3")
  
for (curr_subj in unique(exp.3.only$subject)) {
  curr_data <- exp.3.only %>% 
    filter(subject == curr_subj) %>%
    dplyr::select(target,rt) %>% 
    na.omit() %>%
    group_by(target) %>%
    dplyr::mutate(row_id=1:n()) %>%
    ungroup() %>%  
    spread(key = target, value = rt) %>%
    #ungroup(subject)
    dplyr::select(-row_id) 
  
# find minimum number of rows, N
  n_rows <- curr_data %>% summarise_all(funs(sum(!is.na(.)))) %>% unlist() %>% unname()
# exceptions
  if(curr_subj == "r4893") { # 3/29
    curr_data <- curr_data %>% 
    mutate(tu = as.numeric(rep(NA,nrow(curr_data))))
    N <- sort(n_rows)[2] # select the second lowest length
  } else if(curr_subj == "l6988") { # 3/26
    # subject had only 2 entires in tu, so just make the column NA to preserve other data
    curr_data <- curr_data %>% 
    mutate(tu = as.numeric(rep(NA,nrow(curr_data))))
    N <- sort(n_rows)[2] # select the second lowest length
  } else if (curr_subj == "i3939") { # 3/21
    curr_data <- curr_data %>% 
    mutate(be = as.numeric(rep(NA,nrow(curr_data))))
    N <- sort(n_rows)[2] 
  } else if (curr_subj == "v7564") { # 3/11
    curr_data <- curr_data %>% 
    add_column(ro = as.numeric(rep(NA,nrow(curr_data))))
    n_rows <- curr_data %>% summarise_all(funs(sum(!is.na(.)))) %>% unlist() %>% unname()
    N <- sort(n_rows)[2]
  } else if (curr_subj == "s5965") { # 3/8
    curr_data <- curr_data %>% 
    mutate(po = as.numeric(rep(NA,nrow(curr_data))))
    N <- sort(n_rows)[2] # select the second lowest length
    # and maybe also ga? 
  } else if (curr_subj == "s7540") { # 3/2
    curr_data <- curr_data %>% 
    add_column(be = as.numeric(rep(NA,nrow(curr_data))),.after = "za") %>%
    add_column(ro = as.numeric(rep(NA,nrow(curr_data))),.after = "di")
    n_rows <- curr_data %>% summarise_all(funs(sum(!is.na(.)))) %>% unlist() %>% unname()
    N <- sort(n_rows)[3]
  } else if (curr_subj == "m0085") { # 3/1
    curr_data <- curr_data %>% 
    mutate(be = as.numeric(rep(NA,nrow(curr_data))))
    N <- sort(n_rows)[2] 
  } else {
    N <- min(n_rows)
  }
# create output matrix with N rows
  curr_data.mat <- curr_data[1:N,]
# shuffle longer columns into shorter new matrix
  for (i in 1:length(curr_data.mat)) {
  if (n_rows[i] < N) {curr_data.mat[,i] <- sample(as.matrix(curr_data[1:N,i]), size = N)
  } else {curr_data.mat[,i] <- sample(as.matrix(curr_data[1:n_rows[i],i]), size = N)}
  }

# take correlation(), then transform to z values()
  curr_data.cor <- cor(curr_data.mat) # corr
  curr_data.cor.z <- FisherZ(cor(curr_data.mat)) # corr - fisher
  curr_data.cor.diss.z <- FisherZ(1-cor(curr_data.mat)) # diss - fisher
  curr_data.cor.diss <- 1-(cor(curr_data.mat)) # diss

# append
  exp3.list <- append(exp3.list, list(curr_data.mat)) # Regular matrices, uneven
  exp3.cor.list <- append(exp3.cor.list, list(curr_data.cor)) # Correlations
  exp3.cor.z.list <- append(exp3.cor.z.list, list(curr_data.cor.z)) # Z-transformed correlations
  exp3.cor.diss.z.list <- append(exp3.cor.diss.z.list, list(curr_data.cor.diss.z)) # Z-transformed 1-correlations
  exp3.cor.diss.list <- append(exp3.cor.diss.list, list(curr_data.cor.diss)) # 1-correlations
}

```

Procedure: 
1. Take 1-correlations for each subject, z transform
### Set analysis type
```{r eval=FALSE, include=FALSE}
# Dissimilarity Matrix
# single.analysis <- exp3.cor.diss.list
# single.analysis <- exp3.cor.diss.z.list

# Similarity Matrix
# single.analysis <- exp3.cor.list
 single.analysis <- exp3.cor.z.list

 
```

```{r}
subj_table <- data.frame(subject = as.factor(0),
  test = as.factor(0),
  n.with = as.double(0),
  n.accr = as.double(0),
  median.with = as.double(0),
  median.accr = as.double(0))
 # score = as.double(0))#,
  # w = as.double(0),
  # CI.low = as.double(0),
  # CI.hi = as.double(0),
  # min_p = as.double(0),
  # p = as.double(0))
```

### Compute RSA for all positions
```{r eval=FALSE, include=FALSE}

for (curr.subj in 1:length(single.analysis)) {
  curr.subj.mat <- single.analysis[curr.subj][[1]]

# ------ Ordinal Position

# Collect all possible values
wn.arr <- list()
for (i in 1:length(wn.OP)) {wn.arr <- append(wn.arr, curr.subj.mat[unlist(wn.OP[i])[1],unlist(wn.OP[i])][2])} 
ac.arr <- list()
for (i in 1:length(ac.OP)) {ac.arr <- append(ac.arr, curr.subj.mat[unlist(ac.OP[i])[1],unlist(ac.OP[i])][2])} 
# Wrangle
within.OP <- cbind(wn.arr) %>% as.numeric()
within.OP[mapply(is.infinite, within.OP)] <- NA
#loss <- length(within.OP[is.na(within.OP)])/length(within.OP)
#if (loss > 0.2) {print(paste0("Within OP loss: ",round(loss, digits = 3),"%."))}
within.OP <- within.OP[!is.na(within.OP)] # remove NA
# - 
across.OP <- cbind(ac.arr) %>% as.numeric()
across.OP[mapply(is.infinite, across.OP)] <- NA
#loss <- length(across.OP[is.na(across.OP)])/length(across.OP)
#if (loss > 0.2) {print(paste0("Across OP loss: ",round(loss, digits = 3),"%."))}
across.OP <- across.OP[!is.na(across.OP)] # remove NA
# Compute maximum length
#(n.op <- round(min(length(within.OP),length(across.OP))))
#if(n.op < 2*((1/2)^12)) {"OP min p > 0.0004."}

# Bootstrap/permute
within.OP.median <- median(within.OP) 
across.OP.median <- median(across.OP)


#normed.mu <- within.OP.mean/across.OP.mean

subj_table <- rbind(subj_table, data.frame(subject = unique(exp.3.only$subject)[curr.subj],
                                           test = 'triplet position',
                                           n.with = length(within.OP),
                                           n.accr = length(across.OP),
                                           median.with = within.OP.median,
                                           median.accr = across.OP.median))
                                           #score = normed.mu))
                                           # w = wt.OP$statistic,
                                           # CI.low = wt.OP$conf.int[1],
                                           # CI.hi = wt.OP$conf.int[2],
                                           # min_p = 2*((1/2)^n.op),
                                           # p = wt.OP$p.value))
# ---------------- Transitional Probability

wn.tp.arr <- list()
for (i in 1:length(wn.TP)) {wn.tp.arr <- append(wn.tp.arr, curr.subj.mat[unlist(wn.TP[i])[1],unlist(wn.TP[i])][2])} 
ac.tp.arr <- list()
for (i in 1:length(ac.TP)) {ac.tp.arr <- append(ac.tp.arr, curr.subj.mat[unlist(ac.TP[i])[1],unlist(ac.TP[i])][2])}
within.TP <- cbind(wn.tp.arr) %>% as.numeric()
within.TP[mapply(is.infinite, within.TP)] <- NA
#loss <- length(within.TP[is.na(within.TP)])/length(within.TP)
#if (loss > 0.2) {print(paste0("Within TP loss: ",round(loss, digits = 3),"%."))}
within.TP <- within.TP[!is.na(within.TP)] # remove NA
across.TP <- cbind(ac.tp.arr) %>% as.numeric()
across.TP[mapply(is.infinite, across.TP)] <- NA
#loss <- length(across.TP[is.na(across.TP)])/length(across.TP)
#if (loss > 0.2) {print(paste0("Across TP loss: ",round(loss, digits = 3),"%."))}
across.TP <- across.TP[!is.na(across.TP)] # remove NA

#(n.tp <- round(min(length(within.TP),length(across.TP))))
#if(n.tp < 2*((1/2)^6)) {"TP min p > 0.03."}

#within.TP.samp <- sample(within.TP, size = n.tp, replace = FALSE)
within.TP.median <- median(within.TP) 

#across.TP.samp <- sample(across.TP, size = n.tp, replace = FALSE)
across.TP.median <- median(across.TP) 

#(wt.TP <- wilcox.test(within.TP.samp,across.TP.samp, conf.int = TRUE, conf.level = 0.95, paired = TRUE))
#normed.mu <- within.TP.mean/across.TP.mean

subj_table <- rbind(subj_table, data.frame(subject = unique(exp.3.only$subject)[curr.subj],
                                           test = 'transitional probability',
                                           n.with = length(within.TP),
                                           n.accr = length(across.TP),
                                           median.with = within.TP.median,
                                           median.accr = across.TP.median))
                                           #score = normed.mu))
                                           # w = wt.TP$statistic,
                                           # CI.low = wt.TP$conf.int[1],
                                           # CI.hi = wt.TP$conf.int[2],
                                           # min_p = 2*((1/2)^n.tp),
                                           # p = wt.TP$p.value))

# ---------------- Word Identity

wn.WI.arr <- list()
for (i in 1:length(wn.WI)) {wn.WI.arr <- append(wn.WI.arr, curr.subj.mat[unlist(wn.WI[i])[1],unlist(wn.WI[i])][2])} 
ac.WI.arr <- list()
for (i in 1:length(ac.WI)) {ac.WI.arr <- append(ac.WI.arr, curr.subj.mat[unlist(ac.WI[i])[1],unlist(ac.WI[i])][2])}
within.WI <- cbind(wn.WI.arr) %>% as.numeric()
within.WI[mapply(is.infinite, within.WI)] <- NA
#loss <- length(within.WI[is.na(within.WI)])/length(within.WI)
#if (loss > 0.2) {print(paste0("Within WI loss: ",round(loss, digits = 3),"%."))}
within.WI <- within.WI[!is.na(within.WI)] # remove NA
across.WI <- cbind(ac.WI.arr) %>% as.numeric()
across.WI[mapply(is.infinite, across.WI)] <- NA
#loss <- length(across.WI[is.na(across.WI)])/length(across.WI)
#if (loss > 0.2) {print(paste0("Across WI loss: ",round(loss, digits = 3),"%."))}
across.WI <- across.WI[!is.na(across.WI)] # remove NA

#(n.wi <- round(min(length(within.WI),length(across.WI))))
#if(n.wi < 2*((1/2)^12)) {"OP min p > 0.0004."}

within.WI.median <- median(within.WI)

across.WI.median <- median(across.WI)

#(wt.WI <- wilcox.test(within.WI.samp,across.WI.samp, conf.int = TRUE, conf.level = 0.95, paired = TRUE))

#normed.mu <- within.WI.mean/across.WI.mean

subj_table <- rbind(subj_table, data.frame(subject = unique(exp.3.only$subject)[curr.subj],
                                           test = 'word grouping',
                                           n.with = length(within.WI),
                                           n.accr = length(across.WI),
                                           median.with = within.WI.median,
                                           median.accr = across.WI.median))
                                           #score = normed.mu))
                                           # w = wt.WI$statistic,
                                           # CI.low = wt.WI$conf.int[1],
                                           # CI.hi = wt.WI$conf.int[2],
                                           # min_p = 2*((1/2)^n.wi),
                                           # p = wt.WI$p.value))
# ------------------- Duplet Identity

wn.di.arr <- list()
for (i in 1:length(wn.di)) {wn.di.arr <- append(wn.di.arr, curr.subj.mat[unlist(wn.di[i])[1],unlist(wn.di[i])][2])} 
ac.di.arr <- list()
for (i in 1:length(ac.di)) {ac.di.arr <- append(ac.di.arr, curr.subj.mat[unlist(ac.di[i])[1],unlist(ac.di[i])][2])} 

within.di <- cbind(wn.di.arr) %>% as.numeric()
within.di[mapply(is.infinite, within.di)] <- NA
#loss <- length(within.di[is.na(within.di)])/length(within.di)
#if (loss > 0.2) {print(paste0("Within DI loss: ",round(loss, digits = 3),"%."))}
within.di <- within.di[!is.na(within.di)] # remove NA

across.di <- cbind(ac.di.arr) %>% as.numeric()
across.di[mapply(is.infinite, across.di)] <- NA
#loss <- length(across.di[is.na(across.di)])/length(across.di)
#if (loss > 0.2) {print(paste0("Across DI loss: ",round(loss, digits = 3),"%."))}
across.di <- across.di[!is.na(across.di)] # remove NA

#(n.di <- round(min(length(within.di),length(across.di))))
#if(n.di < 2*((1/2)^6)) {"OP min p > 0.125."}

#within.di.samp <- sample(within.di, size = n.di, replace = FALSE)
within.di.median <- median(within.di)

#across.di.samp <- sample(across.di, size = n.di, replace = FALSE)
across.di.median <- median(across.di)

#(wt.DI <- wilcox.test(within.di.samp,across.di.samp, conf.int = TRUE, conf.level = 0.95, paired = TRUE))

#normed.mu <- within.di.mean/across.di.mean

subj_table <- rbind(subj_table, data.frame(subject = unique(exp.3.only$subject)[curr.subj],
                                           test = 'duplet pairing',
                                           n.with = length(within.di),
                                           n.accr = length(across.di),
                                           median.with = within.di.median,
                                           median.accr = across.di.median))
                                           # score = normed.mu))
                                           # w = wt.DI$statistic,
                                           # CI.low = wt.DI$conf.int[1],
                                           # CI.hi = wt.DI$conf.int[2],
                                           # min_p = 2*((1/2)^n.di),
                                           # p = wt.DI$p.value))

} # subject

subj_table <- subj_table[-1,]
write_csv(subj_table,file.path(res_path,'wilcoxon_subjects-level.csv'))
```



## Load WordRec
```{r}
# Read in file if starting here. 
subj_table <- read_csv(file.path(res_path,'wilcoxon_subjects-level.csv')) %>%
              mutate(subject = as.factor(subject),
                    test = as.factor(test))


data_path <- file.path(code_path,'../../','1_data','exp_3')
wordrec_data_orig <- read_csv(file.path(data_path,'wordrec_task_data.csv')) %>%
    dplyr::select(subj_id:zabetu) %>%
    mutate(subject = as.factor(subj_id), subj_id = NULL) %>% 
    dplyr::select(subject,prop_word_chosen:zabetu)


wordrec_data_corr <- wordrec_data_orig[(wordrec_data_orig$subject) %in% unique(subj_table$subject),] 
subj_table_corr <- subj_table[(subj_table$subject) %in% unique(wordrec_data_corr$subject),] 
length(unique(subj_table_corr$subject))
length(unique(wordrec_data_corr$subject))

corr.table <- subj_table_corr %>% add_column(prop.word = rep(wordrec_data_corr$prop_word_chosen,each = 4))
```

### Create Score
```{r}
corr.table$sub <- corr.table$median.with-corr.table$median.accr
summary(corr.table$sub)
# corr.table$sub.scale <- scale(corr.table$mean.with-corr.table$mean.accr, center = 0)
# summary(corr.table$sub.scale)
# corr.table <- corr.table %>% mutate(sub.scale.or = sub.scale)
# corr.table$sub.scale.or[which(corr.table$sub.scale.or>1 | corr.table$sub.scale.or < -1)] <- NA
# summary(corr.table$sub.scale.or)
# corr.table <- corr.table %>%mutate(log.sub = log(sub.scale + 3))
# summary(corr.table$log.sub)
# corr.table <- corr.table %>%mutate(log.sub.or = log.sub)
# corr.table$log.sub.or[which(corr.table$log.sub.or>1 | corr.table$log.sub.or < -1)] <- NA 
# summary(corr.table$log.sub.or)
```

### -> Correlate
```{r}
(corr.ordp <- cor.test(corr.table$sub[corr.table$test=="triplet position"],
                     corr.table$prop.word[corr.table$test=="triplet position"], 
                     method = "pearson",
                     alternative = "greater"))
#                     alternative = "two.sided"))


(corr.prob <- cor.test(corr.table$sub[corr.table$test=="transitional probability"],
                     corr.table$prop.word[corr.table$test=="transitional probability"], 
                     method = "pearson",
                     alternative = "greater"))
#                     alternative = "two.sided"))

(corr.word <- cor.test(corr.table$sub[corr.table$test=="word grouping"],
                     corr.table$prop.word[corr.table$test=="word grouping"], 
                     method = "pearson",
                     alternative = "greater"))
#                     alternative = "two.sided"))

(corr.dups <- cor.test(corr.table$sub[corr.table$test=="duplet pairing"],
                     corr.table$prop.word[corr.table$test=="duplet pairing"], 
                     method = "pearson",
                     alternative = "greater"))
#                     alternative = "two.sided"))
```

### -> Plot
```{r warning=FALSE}
corr.table$test <- factor(corr.table$test,levels = c("triplet position","transitional probability","word grouping","duplet pairing"))

plots.text <- data.frame(test = factor(c("triplet position","transitional probability","word grouping","duplet pairing"),
                                       levels =c("triplet position","transitional probability","word grouping","duplet pairing")),
  label = c(paste0("\u03C1 = ",round(corr.ordp$estimate, digits = 2),", p = ",round(corr.ordp$p.value, digits = 2)),
             paste0("\u03C1 = ",round(corr.prob$estimate, digits = 2),", p = ",round(corr.prob$p.value, digits = 2)),
             paste0("\u03C1 = ",round(corr.word$estimate, digits = 2),", p = ",round(corr.word$p.value, digits = 2)),
             paste0("\u03C1 = ",round(corr.dups$estimate, digits = 2),", p = ",round(corr.dups$p.value, digits = 2))))
```


```{r warning=FALSE}
# Let's do the whole thing! 
ggplot(corr.table, aes(sub, prop.word, color = test)) + 
  geom_point(size = 2, alpha = 0.8) +
  geom_smooth(method = lm, se = FALSE, size = 1.1) +
 scale_color_manual(values = c("#D95C4E", "#E8B651", "#7BB095", "#5E8FEB"),
                     name = NULL, 
                     guide = NULL) +
  scale_x_continuous(name="within-across similarity z(Pearson's r)") + 
  scale_y_continuous(name="% correct (out of 16)") +
# annotate(geom = "text", x = -0.6, y = 1.1, label = paste0("\u03C1 = ",round(corr.ordp$estimate, digits = 2),", p = ",round(corr.ordp$p.value, digits = 2)),
#         size = 4, color = "#D95C4E") +
# annotate(geom = "text", x = -0.22, y = 1.1, label = paste0("\u03C1 = ",round(corr.prob$estimate, digits = 2),", p = ",round(corr.prob$p.value, digits = 2)),
#         size = 4, color = "#E8B651", fontface = "bold") +
# annotate(geom = "text", x = 0.22, y = 1.1, label = paste0("\u03C1 = ",round(corr.word$estimate, digits = 2),", p = ",round(corr.word$p.value, digits = 2)),
#         size = 4, color = "#7BB095") +
# annotate(geom = "text", x = 0.6, y = 1.1, label = paste0("\u03C1 = ",round(corr.dups$estimate, digits = 2),", p = ",round(corr.dups$p.value, digits = 2)),
#         size = 4, color = "#5E8FEB") +
#---------------------------------------- if facets, plot on each plot the stats
  geom_text(data = plots.text, mapping = aes(x = -Inf, y = -Inf, label = label), color = "black", hjust = -0.1, vjust = -1, size = 5) +
  facet_wrap(. ~ test,nrow = 2)+ 
  geom_vline(xintercept = 0, color = "gray", linetype="dashed") + 
  theme_classic() +
  theme(text = element_text(family = "LM Roman 10", face="bold",size=20), 
        legend.title = element_text(""),  legend.position = "top")# +
  #ggsave(file.path(fig_path,'fig10_corr_sub_final_labels_x_facet2_col.png'),width = 9, height = 7)  



```

